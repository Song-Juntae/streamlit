import streamlit as st
import streamlit.components.v1 as components

import plotly.express as px
import plotly.graph_objects as go

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
import ast
from datetime import datetime
from datetime import timedelta

import warnings
warnings.filterwarnings("ignore", message="PyplotGlobalUseWarning")

from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import koreanize_matplotlib

from PIL import Image

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from gensim.models import Word2Vec
import networkx as nx
import gensim
from pyvis.network import Network
from wordcloud import WordCloud
########################################################################################################################
# ë°ì´í„° ë¡œë“œ ìƒìˆ˜
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼ = pd.read_csv('/app/streamlit/data/ë¦¬ë·°6ì°¨.csv')
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] = pd.to_datetime(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'])
 

ê¸ë¶€ì • = st.radio(
    "ğŸ€ë¦¬ë·° ì„ íƒğŸ€",
    ('All', 'ê¸ì •ë¦¬ë·°ğŸ˜Š', 'ë¶€ì •ë¦¬ë·°ğŸ˜«'), horizontal=True)
if ê¸ë¶€ì • == 'All':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •'))
if ê¸ë¶€ì • == 'ê¸ì •ë¦¬ë·°ğŸ˜Š':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •')
if ê¸ë¶€ì • == 'ë¶€ì •ë¦¬ë·°ğŸ˜«':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •')

option = st.selectbox(
    'ğŸ€ë‹¨ì–´ê¸°ì¤€ì„ íƒğŸ€',
    ('ë¹ˆë„(Count)', 'ì¤‘ìš”ë„(TF-IDF)'))
st.write('ì„ íƒê¸°ì¤€: ', option)


í’ˆì‚¬ì˜µì…˜ = st.selectbox(
    'ğŸ€í’ˆì‚¬ì„ íƒğŸ€',
    ('ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'))
st.write('ì„ íƒí’ˆì‚¬: ', í’ˆì‚¬ì˜µì…˜)


íšŒì‚¬ì¢…ë¥˜ = st.selectbox(
    'ğŸ€ì œí’ˆì„ íƒğŸ€',
    ('ìì‚¬+ê²½ìŸì‚¬', 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„', 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ', 
        'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ', 
        'ê²½ìŸì‚¬-ì‚´ì¶©ì œ',
        'ê²½ìŸì‚¬-ì‹ë¬¼ë“±',
        'ê²½ìŸì‚¬All',
        ))
st.write('ì„ íƒì œí’ˆ: ', íšŒì‚¬ì¢…ë¥˜)
if íšŒì‚¬ì¢…ë¥˜ == 'ìì‚¬+ê²½ìŸì‚¬':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„'))
if íšŒì‚¬ì¢…ë¥˜ == 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„')
if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ì˜ì–‘ì œ'))
if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ë¿Œë¦¬ì˜ì–‘ì œ'))
if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‚´ì¶©ì œ'))
if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ë“±'))
if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬All':
    íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬')

ì‹œì‘ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].min()
ë§ˆì§€ë§‰ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].max()


start_date = st.date_input(
    'ğŸ€ì‹œì‘ë‚ ì§œğŸ€',
    value=ì‹œì‘ë‚ ì§œ,
    min_value=ì‹œì‘ë‚ ì§œ,
    max_value=ë§ˆì§€ë§‰ë‚ ì§œ
)

end_date = st.date_input(
    'ğŸ€ë§ˆì§€ë§‰ë‚ ì§œğŸ€',
    value=ë§ˆì§€ë§‰ë‚ ì§œ,
    min_value=ì‹œì‘ë‚ ì§œ,
    max_value=ë§ˆì§€ë§‰ë‚ ì§œ
)

ê¸°ê°„ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] >= pd.to_datetime(start_date)) & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] <= pd.to_datetime(end_date)))
ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[ê¸ë¶€ì •ë§ˆìŠ¤í¬ & ê¸°ê°„ë§ˆìŠ¤í¬ & íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬]

# í‚¤ì›Œë“œ = st.text_input('ğŸ€ë„¤íŠ¸ì›Œí¬ ë‹¨ì–´ì…ë ¥ğŸ€', 'ì œë¼ëŠ„')

# if í‚¤ì›Œë“œ.find(',') == -1:
#     st.write('ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‹ë¬¼, ì‘ì• ')
#     í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]

# if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬':
#     í’ˆì‚¬ = 'noun'
# if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬':
#     í’ˆì‚¬ = 'n_v_ad'

í‚¤ì›Œë“œ = ['ì‘ì• ', 'ì œë¼ëŠ„', 'ìŠ¤í‚¨ë‹µì„œìŠ¤']

reviews = [eval(i) for i in ë§ˆìŠ¤í¬ëœë°ì´í„°í”„ë ˆì„[í’ˆì‚¬]]
def ë„¤íŠ¸ì›Œí¬(reviews):
    networks = []
    for review in reviews:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=100)

    G = nx.Graph(font_path='/app/streamlit/font/NanumBarunGothic.ttf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in í‚¤ì›Œë“œ:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=20)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in í‚¤ì›Œë“œ:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    # plt.figure(figsize=(15,15))
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, font_family='NanumGothic', with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", font_weight="medium", edge_color="grey", width=edge_weights)

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ë¼ë¦¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶œë ¥
    overlapping_í‚¤ì›Œë“œ = set()
    for i, keyword1 in enumerate(í‚¤ì›Œë“œ):
        for j, keyword2 in enumerate(í‚¤ì›Œë“œ):
            if i < j and keyword1 in G and keyword2 in G:
                if nx.has_path(G, keyword1, keyword2):
                    overlapping_í‚¤ì›Œë“œ.add(keyword1)
                    overlapping_í‚¤ì›Œë“œ.add(keyword2)
    if overlapping_í‚¤ì›Œë“œ:
        print(f"ë‹¤ìŒ ì¤‘ì‹¬ í‚¤ì›Œë“œë“¤ë¼ë¦¬ ì—°ê´€ì„±ì´ ìˆì–´ ì¤‘ë³µë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(overlapping_í‚¤ì›Œë“œ)}")

    net = Network(notebook=True, cdn_resources='in_line')
    net.from_nx(G)
    return [net, similar_words]

ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(reviews)
ë„¤íŠ¸ì›Œí¬

# try:
#     net = ë„¤íŠ¸ì›Œí¬[0]
#     net.save_graph(f'/app/streamlit/pyvis_graph.html')
#     HtmlFile = open(f'/app/streamlit/pyvis_graph.html', 'r', encoding='utf-8')
#     components.html(HtmlFile.read(), height=435)
# except:
#     st.write('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')