import streamlit as st
import streamlit.components.v1 as components

import plotly.express as px
import plotly.graph_objects as go

# ê¸°ë³¸ ë¼ì´ë¸ŒëŸ¬ë¦¬
import os
from datetime import datetime
from datetime import timedelta

from collections import Counter
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import koreanize_matplotlib

from PIL import Image

from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer

from gensim.models import Word2Vec
import networkx as nx
from pyvis.network import Network
from wordcloud import WordCloud
########################################################################################################################
# ë°ì´í„° ë¡œë“œ ìƒìˆ˜
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼ = pd.read_csv('/app/streamlit/data/ë¦¬ë·°6ì°¨.csv')
df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] = pd.to_datetime(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'])

stopwords = ['ì–¸ëŠ˜', 'ê²°êµ­', 'ìƒê°', 'í›„ê¸°', 'ê°ì‚¬', 'ì§„ì§œ', 'ì™„ì „', 'ì‚¬ìš©', 'ìš”ì¦˜', 'ì •ë„', 'ì´ë²ˆ', 'ë‹¬ë¦¬ë·°', 'ê²°ê³¼', 
             'ì§€ê¸ˆ', 'ë™ì˜ìƒ', 'ì¡°ê¸ˆ', 'ì•ˆí…Œ', 'ì…ì œ', 'ì˜ìƒ', 'ì´ë²ˆê±´', 'ë©°ì¹ ', 'ì´ì œ', 'ê±°ì‹œê¸°', 'ì–¼ë“¯', 'ì²˜ìŒ', 'ë‹¤ìŒ']
########################################################################################################################
# title
st.title('ğŸŒ»ì‹ë¬¼ì˜ì–‘ì œ ë¦¬ë·° ë¶„ì„ ëŒ€ì‹œë³´ë“œğŸŒ»')

########################################################################################################################
# ë ˆì´ì•„ì›ƒ
with st.container():
    col0_1, col0_2, col0_3, col0_4, col0_4 = st.columns([1,1,1,1,1])
with st.container():
    col1_1, col1_2, col1_3, col1_4 = st.columns([1,1,1,1])
with st.container():
    col2_1, col2_2, col2_3, col2_4 = st.columns([1,1,1,1])
with st.container():
    col3_1, col3_2 = st.columns([1,1])
with st.container():
    col4_1, col4_2, col4_3 = st.columns([1,1,2])
########################################################################################################################
# ì‚¬ìš©ì ì…ë ¥
with col0_3:
    ê¸ë¶€ì • = st.radio(
    "**ê¸ì •ë¦¬ë·°/ë¶€ì •ë¦¬ë·° ì„ íƒ**",
    ('All', 'ê¸ì •ë¦¬ë·°ğŸ˜Š', 'ë¶€ì •ë¦¬ë·°ğŸ˜«'), horizontal=True)
if ê¸ë¶€ì • == 'All':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •'))
if ê¸ë¶€ì • == 'Positive':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ê¸ì •')
if ê¸ë¶€ì • == 'Negative':
    ê¸ë¶€ì •ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'] == 'ë¶€ì •')

with col1_1:
    option = st.selectbox(
        'ğŸ€ë‹¨ì–´ê¸°ì¤€ì„ íƒğŸ€',
        ('ë¹ˆë„(Count)', 'ì¤‘ìš”ë„(TF-IDF)'))
    st.write('ì„ íƒê¸°ì¤€: ', option)

with col1_2:
    í’ˆì‚¬ì˜µì…˜ = st.selectbox(
        'ğŸ€í’ˆì‚¬ì„ íƒğŸ€',
        ('ëª…ì‚¬', 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬'))
    st.write('ì„ íƒí’ˆì‚¬: ', í’ˆì‚¬ì˜µì…˜)

with col1_3:
    íšŒì‚¬ì¢…ë¥˜ = st.selectbox(
        'ğŸ€ì œí’ˆì„ íƒğŸ€',
        ('ìì‚¬+ê²½ìŸì‚¬', 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„', 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ', 
         'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ', 
         'ê²½ìŸì‚¬-ì‚´ì¶©ì œ',
         'ê²½ìŸì‚¬-ì‹ë¬¼ë“±',
         'ê²½ìŸì‚¬All',
         ))
    st.write('ì„ íƒì œí’ˆ: ', íšŒì‚¬ì¢…ë¥˜)
    if íšŒì‚¬ì¢…ë¥˜ == 'ìì‚¬+ê²½ìŸì‚¬':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') | (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê½ƒí”¼ìš°ëŠ” ì‹œê°„':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê½ƒí”¼ìš°ëŠ”ì‹œê°„')
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ì˜ì–‘ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ì˜ì–‘ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ë¿Œë¦¬ì˜ì–‘ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ë¿Œë¦¬ì˜ì–‘ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‚´ì¶©ì œ':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‚´ì¶©ì œ'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬-ì‹ë¬¼ë“±':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬') & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['item'] == 'ì‹ë¬¼ë“±'))
    if íšŒì‚¬ì¢…ë¥˜ == 'ê²½ìŸì‚¬All':
        íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬ = (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['name'] == 'ê²½ìŸì‚¬')


ì‹œì‘ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].min()
ë§ˆì§€ë§‰ë‚ ì§œ = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'][íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬].max()

with col2_3:
    start_date = st.date_input(
        'ì‹œì‘ë‚ ì§œ',
        value=ì‹œì‘ë‚ ì§œ,
        min_value=ì‹œì‘ë‚ ì§œ,
        max_value=ë§ˆì§€ë§‰ë‚ ì§œ
    )
with col2_4:
    end_date = st.date_input(
        'ë§ˆì§€ë§‰ë‚ ì§œ',
        value=ë§ˆì§€ë§‰ë‚ ì§œ,
        min_value=ì‹œì‘ë‚ ì§œ,
        max_value=ë§ˆì§€ë§‰ë‚ ì§œ
    )

ê¸°ê°„ë§ˆìŠ¤í¬ = ((df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] >= pd.to_datetime(start_date)) & (df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['time'] <= pd.to_datetime(end_date)))

with col2_1:
    ì¶”ê°€ë¶ˆìš©ì–´ = st.text_input('ğŸ€í¬í•¨í•˜ì§€ ì•Šì„ ë‹¨ì–´ì…ë ¥ğŸ€', '')
    if ì¶”ê°€ë¶ˆìš©ì–´ == '':
        st.write('ì˜ˆì‹œ : ì˜ì–‘ì œ, ì‹ë¬¼, ë°°ì†¡')
    if ì¶”ê°€ë¶ˆìš©ì–´ != '':
        st.write('ì œê±°í•œ ë‹¨ì–´: ', ì¶”ê°€ë¶ˆìš©ì–´)

with col2_2:
    ë‹¨ì–´ìˆ˜ = st.slider(
        'ğŸ€ë‹¨ì–´ ìˆ˜ë¥¼ ì¡°ì •í•˜ê¸°ğŸ€',
        10, 300, step=1)
    st.write('ë‹¨ì–´ìˆ˜: ', ë‹¨ì–´ìˆ˜)

if ì¶”ê°€ë¶ˆìš©ì–´.find(',') != -1:
    stopwords.extend([i.strip() for i in ì¶”ê°€ë¶ˆìš©ì–´.split(',')])
if ì¶”ê°€ë¶ˆìš©ì–´.find(',') == -1:
    stopwords.append(ì¶”ê°€ë¶ˆìš©ì–´) 

with col1_4:
    í‚¤ì›Œë“œ = st.text_input('ğŸ€ë„¤íŠ¸ì›Œí¬ ë‹¨ì–´ì…ë ¥ğŸ€', 'ì œë¼ëŠ„')
    if í‚¤ì›Œë“œ.find(',') == -1:
        st.write('ì˜ˆì‹œ : ë¿Œë¦¬, ì œë¼ëŠ„, ì‘ì• ')
        í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
    elif í‚¤ì›Œë“œ.find(',') != -1:
        st.write('ì„¤ì •ëœ ë‹¨ì–´: ', í‚¤ì›Œë“œ)
        í‚¤ì›Œë“œ = [i.strip() for i in í‚¤ì›Œë“œ.split(',')]
    else:
        st.warning(f'{í‚¤ì›Œë“œ}ëŠ” {íšŒì‚¬ì¢…ë¥˜}ì— ì—†ëŠ”ë‹¨ì–´ì…ë‹ˆë‹¤. ë‹¤ë¥¸ ë‹¨ì–´ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”. ì¶”ì²œí‚¤ì›Œë“œ: ì œë¼ëŠ„, ë°°ì†¡')
        # st.write('ë¬¸ì œê°€ ìƒê²¼ì–´ìš”.')
     
########################################################################################################################
def get_count_top_words(df, start_date=None, last_date=None, num_words=10, name=None, 
                        sentiment = None, item = None, source = None, í’ˆì‚¬='noun'):
    if name is not None:
        df = df[df['name'] == name]
    if sentiment is not None:
        df = df[df['sentiment'] == sentiment]
    if item is not None:
        df = df[df['item'] == item]
    if source is not None:
        df = df[df['source'] == source]
    if start_date is None:
        start_date = df['time'].min().strftime('%Y-%m-%d')
    if last_date is None:
        last_date = df['time'].max().strftime('%Y-%m-%d')
    df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]
    count_vectorizer = CountVectorizer(stop_words=stopwords)
    count = count_vectorizer.fit_transform(df[í’ˆì‚¬].values)
    count_df = pd.DataFrame(count.todense(), columns=count_vectorizer.get_feature_names_out())
    count_top_words = count_df.sum().sort_values(ascending=False).head(num_words).to_dict()
    return count_top_words

def get_tfidf_top_words(df, start_date=None, last_date=None, num_words=10, name=None, 
                        sentiment = None, item = None, source = None, í’ˆì‚¬='noun'):
    if name is not None:
        df = df[df['name'] == name]
    if sentiment is not None:
        df = df[df['sentiment'] == sentiment]
    if item is not None:
        df = df[df['item'] == item]
    if source is not None:
        df = df[df['source'] == source]
    if start_date is None:
        start_date = df['time'].min().strftime('%Y-%m-%d')
    if last_date is None:
        last_date = df['time'].max().strftime('%Y-%m-%d')
    df = df[(df['time'] >= start_date) & (df['time'] <= last_date)]
    tfidf_vectorizer = TfidfVectorizer(stop_words=stopwords)
    tfidf = tfidf_vectorizer.fit_transform(df[í’ˆì‚¬].values)
    tfidf_df = pd.DataFrame(tfidf.todense(), columns=tfidf_vectorizer.get_feature_names_out())
    tfidf_top_words = tfidf_df.sum().sort_values(ascending=False).head(num_words).to_dict()
    return tfidf_top_words
########################################################################################################################
if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬':
    í’ˆì‚¬ = 'noun'
if í’ˆì‚¬ì˜µì…˜ == 'ëª…ì‚¬+ë™ì‚¬+í˜•ìš©ì‚¬':
    í’ˆì‚¬ = 'n_v_ad'

ì¹´ìš´íŠ¸ = get_count_top_words(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[ê¸°ê°„ë§ˆìŠ¤í¬ & íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬], num_words=ë‹¨ì–´ìˆ˜, í’ˆì‚¬=í’ˆì‚¬)
tdidf = get_tfidf_top_words(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[ê¸°ê°„ë§ˆìŠ¤í¬ & íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬], num_words=ë‹¨ì–´ìˆ˜, í’ˆì‚¬=í’ˆì‚¬)

if option == 'ì¹´ìš´íŠ¸':
    words = ì¹´ìš´íŠ¸
if option == 'td-idf':
    words = tdidf
########################################################################################################################
# ì‚¬ìš©ì ì…ë ¥í›„ ì‚¬ìš©í•  ë°ì´í„° ì •ë¦¬

########################################################################################################################
# íŒŒì´ì°¨íŠ¸
with col4_1:
    df_íŒŒì´ì°¨íŠ¸ = pd.DataFrame(df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['sentiment'].value_counts())
    pie_chart = go.Figure(data=[go.Pie(labels=list(df_íŒŒì´ì°¨íŠ¸.index), values=df_íŒŒì´ì°¨íŠ¸['count'])])
    st.plotly_chart(pie_chart, use_container_width=True)
with col4_2:
    # st.plotly_chart(words)
    ë°”ì°¨íŠ¸ = go.Figure(data=[go.Bar(x=list(words.keys()), y=list(words.values()))])
    st.plotly_chart(ë°”ì°¨íŠ¸, use_container_width=True)

########################################################################################################################
# ì›Œë“œí´ë¼ìš°ë“œ
with col3_1:
    cand_mask = np.array(Image.open('/app/streamlit/data/circle.png'))
    ì›Œë“œí´ë¼ìš°ë“œ = WordCloud(
        background_color="white", 
        max_words=1000,
        font_path = "/app/streamlit/font/NanumBarunGothic.ttf", 
        contour_width=3, 
        colormap='Spectral', 
        contour_color='white',
        # mask=cand_mask,
        width=800,
        height=400
        ).generate_from_frequencies(words)

    st.image(ì›Œë“œí´ë¼ìš°ë“œ.to_array(), use_column_width=True)
########################################################################################################################
# ë„¤íŠ¸ì›Œí¬ ì°¨íŠ¸

reviews = [eval(i) for i in df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[ê¸°ê°„ë§ˆìŠ¤í¬ & íšŒì‚¬ì¢…ë¥˜ë§ˆìŠ¤í¬][í’ˆì‚¬]]

def ë„¤íŠ¸ì›Œí¬(reviews):
    networks = []
    for review in reviews:
        network_review = [w for w in review if len(w) > 1]
        networks.append(network_review)

    model = Word2Vec(networks, vector_size=100, window=5, min_count=1, workers=4, epochs=100)

    G = nx.Graph(font_path='/app/streamlit/font/NanumBarunGothic.ttf')

    # ì¤‘ì‹¬ ë…¸ë“œë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€
    for keyword in í‚¤ì›Œë“œ:
        G.add_node(keyword)
        # ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ ê°€ì¥ ìœ ì‚¬í•œ 20ê°œì˜ ë‹¨ì–´ ì¶”ì¶œ
        similar_words = model.wv.most_similar(keyword, topn=20)
        # ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì„ ë…¸ë“œë¡œ ì¶”ê°€í•˜ê³ , ì£¼ì–´ì§„ í‚¤ì›Œë“œì™€ì˜ ì—°ê²°ì„  ì¶”ê°€
        for word, score in similar_words:
            G.add_node(word)
            G.add_edge(keyword, word, weight=score)
            
    # ë…¸ë“œ í¬ê¸° ê²°ì •
    size_dict = nx.degree_centrality(G)

    # ë…¸ë“œ í¬ê¸° ì„¤ì •
    node_size = []
    for node in G.nodes():
        if node in í‚¤ì›Œë“œ:
            node_size.append(5000)
        else:
            node_size.append(1000)

    # í´ëŸ¬ìŠ¤í„°ë§
    clusters = list(nx.algorithms.community.greedy_modularity_communities(G))
    cluster_labels = {}
    for i, cluster in enumerate(clusters):
        for node in cluster:
            cluster_labels[node] = i
            
    # ë…¸ë“œ ìƒ‰ìƒ ê²°ì •
    color_palette = ["#f39c9c", "#f7b977", "#fff4c4", "#d8f4b9", "#9ed6b5", "#9ce8f4", "#a1a4f4", "#e4b8f9", "#f4a2e6", "#c2c2c2"]
    node_colors = [color_palette[cluster_labels[node] % len(color_palette)] for node in G.nodes()]

    # ë…¸ë“œì— ë¼ë²¨ê³¼ ì—°ê²° ê°•ë„ ê°’ ì¶”ê°€
    edge_weights = [d['weight'] for u, v, d in G.edges(data=True)]

    # ì„ ì˜ ê¸¸ì´ë¥¼ ë³€ê²½ pos
    plt.figure(figsize=(15,15))
    pos = nx.spring_layout(G, seed=42, k=0.15)
    nx.draw(G, pos, font_family='NanumGothic', with_labels=True, node_size=node_size, node_color=node_colors, alpha=0.8, linewidths=1,
            font_size=9, font_color="black", font_weight="medium", edge_color="grey", width=edge_weights)


    # ì¤‘ì‹¬ ë…¸ë“œë“¤ë¼ë¦¬ ê²¹ì¹˜ëŠ” ë‹¨ì–´ ì¶œë ¥
    overlapping_í‚¤ì›Œë“œ = set()
    for i, keyword1 in enumerate(í‚¤ì›Œë“œ):
        for j, keyword2 in enumerate(í‚¤ì›Œë“œ):
            if i < j and keyword1 in G and keyword2 in G:
                if nx.has_path(G, keyword1, keyword2):
                    overlapping_í‚¤ì›Œë“œ.add(keyword1)
                    overlapping_í‚¤ì›Œë“œ.add(keyword2)
    if overlapping_í‚¤ì›Œë“œ:
        print(f"ë‹¤ìŒ ì¤‘ì‹¬ í‚¤ì›Œë“œë“¤ë¼ë¦¬ ì—°ê´€ì„±ì´ ìˆì–´ ì¤‘ë³µë  ê°€ëŠ¥ì„±ì´ ìˆìŠµë‹ˆë‹¤: {', '.join(overlapping_í‚¤ì›Œë“œ)}")


    net = Network(notebook=True, cdn_resources='in_line')

    net.from_nx(G)

    return [net, similar_words]

ë„¤íŠ¸ì›Œí¬ = ë„¤íŠ¸ì›Œí¬(reviews)


with col3_2:
    try:
        net = ë„¤íŠ¸ì›Œí¬[0]
        net.save_graph(f'/app/streamlit/pyvis_graph.html')
        HtmlFile = open(f'/app/streamlit/pyvis_graph.html', 'r', encoding='utf-8')
        components.html(HtmlFile.read(), height=435)
    except:
        st.write('ì¡´ì¬í•˜ì§€ ì•ŠëŠ” í‚¤ì›Œë“œì˜ˆìš”.')
########################################################################################################################
with col4_3:
    if len(í‚¤ì›Œë“œ) == 1:
        ë³´ì—¬ì¤„df = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['noun'].str.contains(í‚¤ì›Œë“œ[0])]
        st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence', 'noun', 'replace_slang_sentence']])
        í‚¤ì›Œë“œ = [í‚¤ì›Œë“œ]
    elif len(í‚¤ì›Œë“œ) > 1:
        ë³´ì—¬ì¤„df = df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼[df_ë¦¬ë·°_ê°ì„±ë¶„ì„ê²°ê³¼['noun'].str.contains('|'.join(í‚¤ì›Œë“œ))]
        st.dataframe(ë³´ì—¬ì¤„df[['name','sentiment','review_sentence']], use_container_width=True)
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################
########################################################################################################################